{
  "version": 1,
  "snippets": [
    {
      "version": 1,
      "javascript": "return seoSpider.data(document.body.innerText);\\n",
      "name": "Extract Text Content",
      "comments": "",
      "type": "EXTRACTION",
      "actionTimeoutSecs": 1,
      "contentTypes": "text/html"
    },
    {
      "version": 1,
      "javascript": "\\/\\/ Open AI embeddings from page content\\n\\/\\/\\n\\/\\/ IMPORTANT:\\n\\/\\/ You will need to supply your API key below on line 11 which will be stored\\n\\/\\/ as part of your SEO Spider configuration in plain text. Also be mindful if \\n\\/\\/ sharing this script that you will be sharing your API key also unless you \\n\\/\\/ delete it before sharing.\\n\\/\\/ \\n\\/\\/\\n\\nconst OPENAI_API_KEY \u003d \\\u0027YOUR OPENAI KEY HERE\\\u0027;\\nconst userContent \u003d document.body.innerText;\\n    \\nfunction chatGptRequest() {\\n    return fetch(\\\u0027https:\\/\\/api.openai.com\\/v1\\/embeddings\\\u0027, {\\n        method: \\\u0027POST\\\u0027,\\n        headers: {\\n            \\\u0027Authorization\\\u0027: `Bearer ${OPENAI_API_KEY}`,\\n            \\\"Content-Type\\\": \\\"application\\/json\\\",\\n        },\\n        body: JSON.stringify({\\n            model: \\\"text-embedding-3-small\\\",\\n            input: `${userContent}`,\\n            encoding_format: \\\"float\\\",\\n            })\\n    })\\n    .then(response \u003d\u003e {\\n        if (!response.ok) {\\n             return response.text().then(text \u003d\u003e {throw new Error(text)});\\n        }\\n        return response.json();\\n    })\\n    .then(data \u003d\u003e {\\n        console.log(data.data[0].embedding);\\n        return data.data[0].embedding.toString();\\n    });\\n}\\n\\n\\nreturn chatGptRequest()\\n    .then(embeddings \u003d\u003e seoSpider.data(embeddings))\\n    .catch(error \u003d\u003e seoSpider.error(error));\\n\\n",
      "name": "OpenAI Embeddings",
      "comments": "",
      "type": "EXTRACTION",
      "actionTimeoutSecs": 1,
      "contentTypes": "text/html"
    },
    {
      "version": 1,
      "javascript": "const OPENAI_API_KEY \u003d \\\u0027YOUR OPENAI KEY HERE\\\u0027;\\r\\nconst userContent \u003d document.body.innerText;\\r\\n\\r\\nfunction chatGptRequest() {\\r\\n    if (new TextEncoder().encode(userContent).length \u003e 8191) { \\/\\/ Checking byte length approximation for tokens\\r\\n        \\/\\/ Function to break the string into chunks\\r\\n        function chunkString(str, size) {\\r\\n            const numChunks \u003d Math.ceil(str.length \\/ size);\\r\\n            const chunks \u003d new Array(numChunks);\\r\\n\\r\\n            for (let i \u003d 0, o \u003d 0; i \u003c numChunks; ++i, o +\u003d size) {\\r\\n                chunks[i] \u003d str.substring(o, o + size);\\r\\n            }\\r\\n            return chunks;\\r\\n        }\\r\\n\\r\\n        \\/\\/ Divide content into manageable chunks\\r\\n        const chunks \u003d chunkString(userContent, 8191);\\r\\n\\r\\n        \\/\\/ Function to request batch embeddings for all chunks\\r\\n        function chatGptBatchRequest(chunks) {\\r\\n            return fetch(\\\u0027https:\\/\\/api.openai.com\\/v1\\/embeddings\\\u0027, {\\r\\n                method: \\\u0027POST\\\u0027,\\r\\n                headers: {\\r\\n                    \\\u0027Authorization\\\u0027: `Bearer ${OPENAI_API_KEY}`,\\r\\n                    \\\"Content-Type\\\": \\\"application\\/json\\\",\\r\\n                },\\r\\n                body: JSON.stringify({\\r\\n                    model: \\\"text-embedding-3-small\\\",\\r\\n                    input: chunks,\\r\\n                    encoding_format: \\\"float\\\",\\r\\n                })\\r\\n            })\\r\\n            .then(response \u003d\u003e {\\r\\n                if (!response.ok) {\\r\\n                    return response.text().then(text \u003d\u003e { throw new Error(text); });\\r\\n                }\\r\\n                return response.json();\\r\\n            })\\r\\n            .then(data \u003d\u003e {\\r\\n                if (data.data.length \u003e 0) {\\r\\n                    const numEmbeddings \u003d data.data.length;\\r\\n                    const embeddingLength \u003d data.data[0].embedding.length;\\r\\n                    const sumEmbedding \u003d new Array(embeddingLength).fill(0);\\r\\n\\r\\n                    data.data.forEach(embed \u003d\u003e {\\r\\n                        embed.embedding.forEach((value, index) \u003d\u003e {\\r\\n                            sumEmbedding[index] +\u003d value;\\r\\n                        });\\r\\n                    });\\r\\n\\r\\n                    const averageEmbedding \u003d sumEmbedding.map(sum \u003d\u003e sum \\/ numEmbeddings);\\r\\n                    return averageEmbedding.toString();\\r\\n                } else {\\r\\n                    throw new Error(\\\"No embeddings returned from the API.\\\");\\r\\n                }\\r\\n            });\\r\\n        }\\r\\n\\r\\n        \\/\\/ Make a single batch request with all chunks and process the average\\r\\n        return chatGptBatchRequest(chunks);\\r\\n    } else {\\r\\n        \\/\\/ Process single embedding request if content is within the token limit\\r\\n        return fetch(\\\u0027https:\\/\\/api.openai.com\\/v1\\/embeddings\\\u0027, {\\r\\n            method: \\\u0027POST\\\u0027,\\r\\n            headers: {\\r\\n                \\\u0027Authorization\\\u0027: `Bearer ${OPENAI_API_KEY}`,\\r\\n                \\\"Content-Type\\\": \\\"application\\/json\\\",\\r\\n            },\\r\\n            body: JSON.stringify({\\r\\n                model: \\\"text-embedding-3-small\\\",\\r\\n                input: userContent,\\r\\n                encoding_format: \\\"float\\\",\\r\\n            })\\r\\n        })\\r\\n        .then(response \u003d\u003e {\\r\\n            if (!response.ok) {\\r\\n                 return response.text().then(text \u003d\u003e {throw new Error(text)});\\r\\n            }\\r\\n            return response.json();\\r\\n        })\\r\\n        .then(data \u003d\u003e {\\r\\n            console.log(data.data[0].embedding);\\r\\n            return data.data[0].embedding.toString();\\r\\n        });\\r\\n    }\\r\\n}\\r\\n\\r\\n\\/\\/ Execute request and handle results\\r\\nreturn chatGptRequest()\\r\\n    .then(embeddings \u003d\u003e seoSpider.data(embeddings))\\r\\n    .catch(error \u003d\u003e seoSpider.error(error));\\r\\n",
      "name": "OpenAI Embeddings Long Inputs",
      "comments": "",
      "type": "EXTRACTION",
      "actionTimeoutSecs": 1,
      "contentTypes": "text/html"
    },
    {
      "version": 1,
      "javascript": "const userContent \u003d document.body.innerText;\\r\\n\\r\\nfunction vertextAiRequest() {\\r\\n    return fetch(\\\u0027http:\\/\\/127.0.0.1:5000\\/embed\\\u0027, {\\r\\n        method: \\\u0027POST\\\u0027,\\r\\n        headers: {\\r\\n            \\\"Content-Type\\\": \\\"application\\/json\\\",\\r\\n        },\\r\\n        body: JSON.stringify({\\r\\n            task: \\\"RETRIEVAL_DOCUMENT\\\",\\r\\n            text: `${userContent}`\\r\\n            })\\r\\n    })\\r\\n    .then(response \u003d\u003e {\\r\\n        if (!response.ok) {\\r\\n             return response.text().then(text \u003d\u003e {throw new Error(text)});\\r\\n        }\\r\\n        return response.json();\\r\\n    })\\r\\n    .then(data \u003d\u003e {\\r\\n        return data.embedding.toString();\\r\\n    });\\r\\n}\\r\\n\\r\\n\\r\\nreturn vertextAiRequest()\\r\\n    .then(embeddings \u003d\u003e seoSpider.data(embeddings))\\r\\n    .catch(error \u003d\u003e seoSpider.error(error));\\r\\n\\r\\n",
      "name": "REAL Vertex Local",
      "comments": "",
      "type": "EXTRACTION",
      "actionTimeoutSecs": 1,
      "contentTypes": "text/html"
    },
    {
      "version": 1,
      "javascript": "const userContent \u003d document.body.innerText;\\n\\nfunction getEmbeddings(userContent) {\\n    const apiUrl \u003d \\\u0027http:\\/\\/localhost:11434\\/api\\/embeddings\\\u0027;\\n\\n    const postData \u003d {\\n        \\\"model\\\": \\\"avr\\/sfr-embedding-mistral\\\",\\n        \\\"prompt\\\": userContent,\\n    };\\n\\n    const fetchOptions \u003d {\\n        method: \\\u0027POST\\\u0027,\\n        headers: {\\n            \\\u0027Content-Type\\\u0027: \\\u0027application\\/json\\\u0027\\n        },\\n        body: JSON.stringify(postData)\\n    };\\n\\n    return fetch(apiUrl, fetchOptions)\\n        .then(response \u003d\u003e {\\n            if (!response.ok) {\\n                return response.text().then(text \u003d\u003e {throw new Error(text)});\\n            }\\n            return response.json();\\n        })\\n        .then(data \u003d\u003e {\\n            return data.embedding;\\n        });\\n}\\n\\nreturn getEmbeddings(userContent)\\n  .then(embeddings \u003d\u003e seoSpider.data(embeddings))\\n  .catch(error \u003d\u003e seoSpider.error(error));\\n  ",
      "name": "ollama SFR Embeddings",
      "comments": "",
      "type": "EXTRACTION",
      "actionTimeoutSecs": 1,
      "contentTypes": "text/html"
    },
    {
      "version": 1,
      "javascript": "// TextRazor Entity Extraction\\n// IMPORTANT:\\n// You will need to supply your API key below which will be stored\\n// as part of your SEO Spider configuration in plain text. \\n// TextRazor's free plan is limited to 500 requests per day.\\n// TextRazor's free plan is capped to two concurrent requests, a delay is built in.\\n\\n// Dennis Stopa / Nodeyn :: https://dennisstopa.com / https://nodeyn.nl\\n\\n// API key and language override variables (keep your API key static in Screaming Frog)\\nconst TEXTRAZOR_API_KEY = '';\\nconst languageOverride = ''; // Leave blank if you don't want to override. Check https://www.textrazor.com/languages for supported languages\\n\\n// No more modifications needed\\nconst userContent = document.body.innerText; \\n\\nlet requestCounter = 0; // Initialize request counter\\nconst maxRequestsPerDay = 500; // Set the maximum requests per day, the free plan has a 500 requests per day\\n\\n// The free plan has a limit of two concurrent requests, the delay will handle this\\nfunction delay(ms) {\\n    return new Promise(resolve => setTimeout(resolve, ms));\\n}\\n\\nasync function extractEntitiesWithDelay() {\\n    const entities = [];\\n    const chunkSize = 5000;\\n    const textChunks = [];\\n\\n    for (let i = 0; i < userContent.length; i += chunkSize) {\\n        textChunks.push(userContent.substring(i, i + chunkSize));\\n    }\\n\\n    for (let i = 0; i < textChunks.length; i++) {\\n        if (requestCounter >= maxRequestsPerDay) {\\n            console.log('Reached the maximum number of requests for the day.');\\n            break;\\n        }\\n\\n        const text = textChunks[i];\\n        console.log('Sending text chunk to TextRazor:', text.slice(0, 200)); \\n\\n        const bodyParams = new URLSearchParams({\\n            text: text,\\n            extractors: 'entities,topics',\\n        });\\n\\n        // Conditionally add the language override if it's provided\\n        if (languageOverride) {\\n            bodyParams.append('languageOverride', languageOverride);\\n        }\\n\\n        const response = await fetch('https://api.textrazor.com/', {\\n            method: 'POST',\\n            headers: {\\n                'x-textrazor-key': TEXTRAZOR_API_KEY, \\n                'Content-Type': 'application/x-www-form-urlencoded'\\n            },\\n            body: bodyParams.toString()\\n        });\\n\\n        if (response.ok) {\\n            const data = await response.json();\\n            console.log('TextRazor response:', data); // Log the response for debugging\\n\\n            if (data.response.entities) {\\n                entities.push(...data.response.entities);\\n                requestCounter++;\\n            }\\n        } else {\\n            const errorText = await response.text();\\n            console.error('TextRazor API error:', errorText);\\n        }\\n\\n        if (i < textChunks.length - 1) {\\n            await delay(1000);\\n        }\\n    }\\n\\n    return entities;\\n}\\n\\nfunction isValidEntity(entity) {\\n    const invalidTypes = ['Number', 'Cookie', 'Email', 'Date'];\\n    const entityId = entity.entityId || entity.matchedText;\\n\\n    if (entity.type && Array.isArray(entity.type) && entity.type.length > 0) {\\n        if (invalidTypes.includes(entity.type[0]) || /^[0-9]+$/.test(entityId)) {\\n            return false;\\n        }\\n    } else if (/^[0-9]+$/.test(entityId)) {\\n        return false;\\n    }\\n\\n    return true;\\n}\\n\\nfunction processEntities(entities) {\\n    const entitiesDict = {};\\n\\n    entities.forEach(entity => {\\n        if (isValidEntity(entity)) {\\n            const entityId = entity.entityId || entity.matchedText;\\n            const entityName = entity.matchedText.toLowerCase(); // Convert entity name to lowercase\\n            const freebaseLink = entity.freebaseId ? `https://www.google.com/search?kgmid=${entity.freebaseId}` : '';\\n            const wikiLink = entity.wikiLink || ''; // Ensure we're capturing the Wikipedia link correctly\\n\\n            if (entityId !== 'None' && isNaN(entityName)) {  // Filter out numeric-only entities\\n                const key = entityName + freebaseLink; // Unique key based on name and link\\n                if (!entitiesDict[key]) {\\n                    entitiesDict[key] = {\\n                        entity: entityName,\\n                        count: 1,\\n                        freebaseLink: freebaseLink,\\n                        wikiLink: wikiLink\\n                    };\\n                } else {\\n                    entitiesDict[key].count += 1;\\n                }\\n            }\\n        }\\n    });\\n\\n    const result = Object.values(entitiesDict).filter(item => item.entity && item.entity !== 'None'); // Filter out empty or 'None' entities\\n\\n    return JSON.stringify(result);\\n}\\n\\nreturn extractEntitiesWithDelay()\\n    .then(entities => {\\n        if (entities.length === 0) {\\n            console.warn('No entities found in the response.');\\n        }\\n        return seoSpider.data(processEntities(entities));\\n    })\\n    .catch(error => seoSpider.error(error));\\n",
      "name": "TextRazor Entity Extraction",
      "comments": "",
      "type": "EXTRACTION",
      "actionTimeoutSecs": 1,
      "contentTypes": "text/html"
    }
  ]
}